---
title: "Big5 and COVID19 replication"
author: "Amit Levinson"
date: "3/23/2020"
output: github_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

#### Introduction
As part of [Almog Simchon's](https://almogsi.com/) "Psychological Data Science Lab", he asked participants to try and replicate  [Tyler Burleigh's fantastic 'COVID-19 and the Big 5 Personality Test' blogpost](https://tylerburleigh.com/blog/covid-19-and-the-big-5-personality-test/)  in `R`.  You can find links to all the data files in his blogpost.
I found parts of it challenging (using a for loop with ggplot) but fun nonetheless! I'd love to hear comments about the code (for example moving the for loop to `apply`) or any mistake I might have done. 

### Libraries

```{r}
library(tidyverse)
library(here)
library(janitor)
```

### Load data

```{r}
train <- read_csv(here("02_data/train.csv"))
test <- read_csv(here("02_data/test.csv"))

covid_raw <- train %>% 
  full_join(test, by = c("Country/Region" = "Country/Region", "Date" = "Date")) 

covid19 <- covid_raw %>% 
  select("Id","Country/Region", "Date", "ConfirmedCases") %>% 
  clean_names()
```

### Filter
Next we'll filter to countries that reached at least 50 cases and had at least 14 days of data beyond that point.

```{r}
covid_mindays <- covid19 %>% 
  filter(confirmed_cases > 50) %>% 
  group_by(country_region) %>% 
  mutate(distinct_dates = n_distinct(date)) %>% 
  filter(distinct_dates >=14) %>% 
  ungroup() %>% 
  select(-distinct_dates)

n_distinct(covid_mindays$country_region)
unique(covid_mindays$country_region)
```

**Why do I have more countries than Tyler?** That's because I downloaded the data on March 23, 2020, while the author of the post downloaded it on March 20, 2020. during these three days some of the countries discovered more cases which, sadly, made them meet our criteria.[^note]

[^note]: Actually, this drove me a bit crazy as I was having trouble figuring this out. Sometimes you just have to open that csv and give it a scroll!

### Compute growth over 14 days

Next we'll compute the growth in cases for each country with our start point being the date they reached 50 confirmed cases (as filtered before) until the following 14 day following that date. 

I'm not sure I'm correctly following along but at this point the original post has some sort of 'index' column that was evident in their display. This column is in the original dataset before we select our columns. Therefore I'll return to that dataset and take the new columns along with the 'index'.


Now let's look at countries that are represented multiple times, probably because they're written under different provinces:
```{r}
head(covid_mindays[covid_mindays$country_region == "China",])
```

```{r}
covid19_collapse_province <- covid_mindays %>% 
  group_by(country_region, date) %>% 
  summarise(sum(confirmed_cases))

head(covid19_collapse_province[covid19_collapse_province$country_region == "China",])
```

```{r}
covid19_df <- covid19_collapse_province %>% 
  group_by(country_region) %>% 
  slice(14) %>% 
  arrange(desc(date)) %>% 
  slice(1)

covid19_df
```

Great, now let's join the country names with country abbreviations (two letters) using the 'wikipedia-iso-country-codes.csv'. I found this more suitable, in accordance to Tyler's post, after trying several attempts with the `{isocodes}`. I only selected the two first columns which contain the Country and Alpha_2 code (the two letters) and reset their names:

```{r}
country_code <- read_csv(here("/02_data/Wikipedia-iso-country-codes.csv")) %>% 
  select(1:2) %>% 
  set_names("country_region", "alpha_2")
```

```{r}
covid19_df <- covid19_df %>% 
  left_join(country_code) %>% 
  clean_names()

head(covid19_df)
```

## Big Five Personality Data

Next let's bring in the [Big Five personality data from kaggla](https://www.kaggle.com/tunguz/big-five-personality-test). The dataset containes 1M answers collected online by [Open Psychometrics](https://openpsychometrics.org/tests/IPIP-BFFM/) and consists of the country in which the respondent is located. 

```{r}
big5 <- read_delim(here("/02_data/data-final.csv"), delim = "\t")
```


I'm not too familiar with the Big 5 so I won't try to explain the underlying factors, positive and negative scales underlying it. You can read more about it in [Tyler's original post](https://tylerburleigh.com/blog/covid-19-and-the-big-5-personality-test/) or more in depth at the scale documentation on the [IPIP's website](https://ipip.ori.org/newBigFive5broadKey.htm), also linked from his post.

### Reverse-coding

Before we begin analyzing the personality test we want to reverse code some of the variables. To do that we just implement a simple substation of 6 from every reverse-keyed item:

```{r}
positively_keyed <- c('EXT1', 'EXT3', 'EXT5', 'EXT7', 'EXT9','EST1', 'EST3', 'EST5', 'EST6', 'EST7', 'EST8', 'EST9', 'EST10','AGR2', 'AGR4', 'AGR6', 'AGR8', 'AGR9', 'AGR10','CSN1', 'CSN3', 'CSN5', 'CSN7', 'CSN9', 'CSN10', 'OPN1', 'OPN3', 'OPN5', 'OPN7', 'OPN8', 'OPN9', 'OPN10')

negatively_keyed <- c('EXT2', 'EXT4', 'EXT6', 'EXT8', 'EXT10', 'EST2', 'EST4',  'AGR1', 'AGR3', 'AGR5', 'AGR7',  'CSN2', 'CSN4', 'CSN6', 'CSN8', 'OPN2', 'OPN4', 'OPN6')
```

```{r}
big5[,negatively_keyed] <- 6-big5[,negatively_keyed]
```

### Country-Level Big 5 Aggregates
First, we'll eliminate countries with not enough observations, having the cutoff be N = 1000:

```{r}
big5_filtered <- big5 %>% 
  group_by(country) %>% 
  filter(n() > 1000)

length(unique(big5_filtered$country))
unique(big5_filtered$country)
```

### Compute average

This is by now way an efficient way but for now it'll have to hold:

```{r}
big5$ext_mean <- rowMeans(big5[,1:10])
big5$est_mean <- rowMeans(big5[,11:20])
big5$agr_mean <- rowMeans(big5[,21:30])
big5$csn_mean <- rowMeans(big5[,31:40])
big5$opn_mean <- rowMeans(big5[,41:50])

# big5[,cols_to_change] <- sapply(big5[,cols_to_change], as.numeric)
```


### Country level averages:
```{r}
country_average <- big5 %>% 
  group_by(country) %>% 
  summarise_at(vars(ext_mean:opn_mean), mean, na.rm = T) %>% 
  ungroup()
```


### Let's look at the top level country extraverts:
```{r}
country_average %>% 
  select(country, ext_mean) %>% 
  arrange(ext_mean) %>% 
  tail() %>% 
  ggplot()+
  geom_col(aes(x = country, y = ext_mean))+
  coord_flip()
```


### Merge tables and plot

```{r}
merged_tables <- country_average %>% 
  inner_join(covid19_df, by = c("country" = "alpha_2"))
```


```{r fig.width=5, fig.height=16}
merged_tables <- merged_tables %>% 
  select(2:6, everything())

factor_nms <- c('Extraversion', 'Emotional Stability', 'Agreeableness', 'Conscientiousness', 'Openness')
x_nms <- c("EXT", "EST", "AGR", "CSN", "OPN")

plots <- list()

for(i in 1:5) {
  # correlation
  corr <- cor.test(merged_tables$sum_confirmed_cases, merged_tables[[i]])
  cor_size <- format(round(corr$estimate, 2), nsmall = 2)
  cor_p <- format(round(corr$p.value,2),nsmall = 2)
  
  # Plot
  plots[[i]] <- local({
    ggplot(merged_tables, aes(x = merged_tables[[i]], y = sum_confirmed_cases))+
      geom_point()+
      geom_smooth(method = "lm")+
      labs(title = glue::glue("Confirmed cases at 14 days after first 50 cases by \naverage score on Big 5 Factor {factor_nms[i]} \nr = {cor_size} p = {cor_p}"), y = "confirmed", x = glue::glue("{x_nms[i]}"))
  })
 }

cowplot::plot_grid(plotlist = plots, nrow = 5)
```


China might be an outlier in this case, since that's where the outbreak started.  
Let's see the plots again without china:

```{r fig.width=5, fig.height=16}
merged_tables_nocn <- merged_tables %>% 
  filter(country != "CN")

for(i in 1:5) {
  # correlation
  corr <- cor.test(merged_tables_nocn$sum_confirmed_cases, merged_tables_nocn[[i]])
  cor_size <- format(round(corr$estimate, 2), nsmall = 2)
  cor_p <- format(round(corr$p.value,2),nsmall = 2)
  
  # Plot
  plots[[i]] <- local({
    ggplot(merged_tables_nocn, aes(x = merged_tables_nocn[[i]], y = sum_confirmed_cases))+
      geom_point()+
      geom_smooth(method = "lm")+
      labs(title = glue::glue("Confirmed cases at 14 days after first 50 cases by \naverage score on Big 5 Factor {factor_nms[i]}\n r = {cor_size} p = {cor_p}"), y = "confirmed", x = glue::glue("{x_nms[i]}"))
  })
 }

cowplot::plot_grid(plotlist = plots, nrow = 5)
```

Its seems like only 'OPN' (openness) showed a pattern. Countries with higher levels of openness saw a higher growth of confirmed cases along the 14 days.

```{r}
merged_tables %>% 
  select(country_region, opn_mean, sum_confirmed_cases) %>% 
  arrange(-opn_mean) %>% 
  knitr::kable() %>% 
  kableExtra::kable_styling()
```

<br>
<br>
<br>

------

### Appendix  

#### Benchmarking data.table::fread, read_delim and read.csv

I initially used `read.csv` to read the big5 file which was slow in loading and data wrangling. Learning (this past week) about data.table was a great surprise to see how much faster it loads. I also eventually figured out to use `read_delim` instead of `read_csv` which eventually worked with the current data.   
Checkout the benchmarking below (I won't be running it, see plot below as image):

```{r eval = FALSE}
results <- microbenchmark::microbenchmark(
big5_read.csv <- read.csv(here("/02_data/data-final.csv"), head = T, sep = "\t", stringsAsFactors = F),
big5_fread <- fread(here("/02_data/data-final.csv")),
big5_read_csv <- read_delim(here("/02_data/data-final.csv"), delim = "\t"),
times = 10
)
# Plot benchmark
boxplot(results)
# Save
# png(file="saving_plot2.png",
# width=600, height=350)
# boxplot(results, names = c("read.csv", "data.table::fread", "readr::read_delim"), main = "Reading 1.8M row file (10 iterations)", ylab = "seconds")
# dev.off()
```

```{r echo = FALSE}
knitr::include_graphics(here("/fig/benchmark_big5.png"))
```

